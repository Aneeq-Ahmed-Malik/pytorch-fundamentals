{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8HgLDNxJyWH"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Tensor data type"
      ],
      "metadata": {
        "id": "prKE5vKeLQFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1, 2, 3])\n",
        "y = torch.rand(3, 3)"
      ],
      "metadata": {
        "id": "U2EkJHyaJ2i5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"x = {x}\")\n",
        "print(f\"y = {y}\")\n",
        "print(f\"x + y = {x + y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVtznG0YKzzL",
        "outputId": "c48563ba-b9d5-4235-9f43-1a6c0027b8be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = tensor([1, 2, 3])\n",
            "y = tensor([[0.7508, 0.0624, 0.2065],\n",
            "        [0.2747, 0.4145, 0.3729],\n",
            "        [0.9187, 0.1907, 0.5265]])\n",
            "x + y = tensor([[1.7508, 2.0624, 3.2065],\n",
            "        [1.2747, 2.4145, 3.3729],\n",
            "        [1.9187, 2.1907, 3.5265]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([[3,4]])\n",
        "x.shape\n",
        "## a matrix of 1 rows and 2 cols"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tfjjk88KdHFn",
        "outputId": "ecd3888d-2e59-47b3-9bd8-142373fc06e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some imp functions\n",
        "\n",
        "1. torch.stack"
      ],
      "metadata": {
        "id": "6rYoSNWmaalU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([1, 2])\n",
        "b = torch.tensor([3, 4])\n",
        "\n",
        "# Stack along a new dimension (dim=0)\n",
        "# [a, b] = [[1,2], [3,4]] but which dim should be new axis??\n",
        "c = torch.stack([a, b], dim=0)  ## dim = 0, means new axis will be rows\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5u1-RBWUa6aA",
        "outputId": "924abab5-f37f-4424-8f0a-ace8c8d3898d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2],\n",
            "        [3, 4]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c = torch.stack([a, b], dim=1) ## dim = 1, means new axis will be cols\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-FwohG9a9-a",
        "outputId": "5d05b6d5-51a2-4e8b-ab76-b52791bb0159"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 3],\n",
            "        [2, 4]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. torch.cat"
      ],
      "metadata": {
        "id": "h4FGQfccbXBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[1, 2]])\n",
        "b = torch.tensor([[3, 4]])\n",
        "\n",
        "c = torch.cat([a, b], dim=0)  # concatenate along rows\n",
        "print(c)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4-Gcf7BbaM1",
        "outputId": "233c71fd-bc86-4d75-aed0-28738b4bd1e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2],\n",
            "        [3, 4]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c = torch.cat([a, b], dim=1)\n",
        "print(c)\n",
        "\n",
        "## cat merges along existing axis\n",
        "## stack adds a new axis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sx03oFQqbf3v",
        "outputId": "5f4fc3a7-cf67-43e2-8693-9e47976caf85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2, 3, 4]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. torch.unsqueeze"
      ],
      "metadata": {
        "id": "AV6I49j-bon3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([1, 2])\n",
        "b = torch.tensor([3, 4])\n",
        "\n",
        "print(f\"a.shape = {a.shape}\")\n",
        "# Unsqueeze to add a new dimension\n",
        "a2 = a.unsqueeze(0)  # shape [1, 2]\n",
        "b2 = b.unsqueeze(0)  # shape [1, 2]\n",
        "\n",
        "print(f\"a2 = {a2}\")\n",
        "print(f\"a2.shape = {a2.shape}\")\n",
        "\n",
        "c = torch.cat([a2, b2], dim=0)\n",
        "print(c)\n",
        "\n",
        "# Looks similar to stack, but here we manually added a dimension before concatenating."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivMHldl8brjF",
        "outputId": "1cdeaa47-de3b-4cd2-db8a-9cae1a32e9eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a.shape = torch.Size([2])\n",
            "a2 = tensor([[1, 2]])\n",
            "a2.shape = torch.Size([1, 2])\n",
            "tensor([[1, 2],\n",
            "        [3, 4]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Computation"
      ],
      "metadata": {
        "id": "pK4PLIWoLIM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([2.0], requires_grad=True)  ## for gradient, tensor must be float / complex\n",
        "y = x ** 2"
      ],
      "metadata": {
        "id": "kI7oyX88K0Xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"x = {x}\")\n",
        "print(f\"y = {y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0Z61A9fLcwy",
        "outputId": "72b043fe-2b06-4a00-c931-ef97c4d7841c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = tensor([2.], requires_grad=True)\n",
            "y = tensor([4.], grad_fn=<PowBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()  ## computes grad, more detail to follow"
      ],
      "metadata": {
        "id": "o7KxXbO1Lggn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.grad)  ## this means that dy/dx at x=2; is 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vll57SC3MDuw",
        "outputId": "e186e053-e6b3-476f-9e4a-b929a5c0b988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([4.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Now suppose we want to check grad at many points\n",
        "x = torch.tensor([2.0, 4.0, 6.0], requires_grad=True)\n",
        "y = x ** 2"
      ],
      "metadata": {
        "id": "9g4u25SeOnbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Inputs: $x_{0} , x_{1}$\n",
        "    \n",
        "- Outputs: $y_{0} = x_{0}^{2} , y_{1} = x_{1}^{2}$​\n",
        "    \n",
        "- Total “loss” if you sum:\n",
        "    \n",
        "\n",
        "$$L = y_{0} + y_{1} = x_{0}^{2} + x_{1}^{2}$$\n",
        "\n",
        "- Then the **gradients w.r.t each input** are:\n",
        "    \n",
        "\n",
        "$$\\frac{\\partial L}{\\partial x_{0}} = 2 x_{0} , \\frac{\\partial L}{\\partial x_{1}} = 2 x_{1}$$"
      ],
      "metadata": {
        "id": "ObN3phEHTqT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## if you simple do y.backward() it fails\n",
        "## at this point, for each x; y = [4.0, 16.0, 36.0]\n",
        "## but we want der on each single point, not for a vector, so we sum up y\n",
        "## sum = x0**2 + x1**2 + x2**2, now we cal der, so it computes for each point separtely\n",
        "\n",
        "sum = y.sum()\n",
        "## it will take dy/dx for each x{t} where t belongs to [0,1,2]\n",
        "sum.backward()"
      ],
      "metadata": {
        "id": "fs9qy9elPbVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.grad)"
      ],
      "metadata": {
        "id": "NDJNmDqIeu2g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "572f6f7e-975f-43db-d7e5-0bc1c0d6d7eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 4.,  8., 12.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1\\. What is a computation graph?**\n",
        "\n",
        "A **computation graph** is a **directed graph that represents all operations performed to compute a tensor**.\n",
        "\n",
        "- **Nodes** = tensors or operations\n",
        "    \n",
        "- **Edges** = dependencies (which tensor was used to compute which)\n",
        "    \n",
        "\n",
        "It’s how PyTorch knows **how to compute gradients automatically**.\n",
        "\n",
        "As in this example,\n",
        "\n",
        "    x --[square]--> y --[maybe other ftns like -3]--> z\n",
        "\n",
        "x → y → z\n",
        "\n",
        "Each operation is a node. PyTorch tracks this automatically.\n"
      ],
      "metadata": {
        "id": "ytGBSheGd3Vb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2\\. Why it’s important**\n",
        "\n",
        "When you call z.backward(), PyTorch traverses the graph in reverse to compute gradients w.r.t all tensors that have requires_grad=True.\n",
        "\n",
        "This is called reverse-mode automatic differentiation."
      ],
      "metadata": {
        "id": "LgC_I9IqedU2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3\\. Multiple outputs**\n",
        "\n",
        "\n",
        "`y = torch.stack([x**2, x**3])`\n",
        "\n",
        "- Graph now looks like:\n",
        "        \n",
        "              _ x _\n",
        "           /         \\\n",
        "          /           \\\n",
        "        [square]    [cube]\n",
        "           |           |\n",
        "         y[0]         y[1]\n",
        "\n",
        "- Each output depends on `x`.\n",
        "    \n",
        "- To compute gradients for both `y[0]` and `y[1]`, we need to **retain the graph** because PyTorch would normally free it after the first `.backward()`."
      ],
      "metadata": {
        "id": "cIeeahsMe2CX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "y = torch.stack([x**2, x**3])\n",
        "print(f\"y = {y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WH8BDzM6f-DT",
        "outputId": "0a6499d2-1014-471b-dbd5-3025284f1178"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y = tensor([[ 4.,  9.],\n",
            "        [ 8., 27.]], grad_fn=<StackBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grads = []\n",
        "\n",
        "for i in range(y.shape[0]):\n",
        "    x.grad = None              # reset gradient\n",
        "    sum = y[i].sum()            ## make them into a single loss ftn\n",
        "    sum.backward(retain_graph=True)  ## retain the computation graph\n",
        "    grads.append(x.grad.clone())\n",
        "\n",
        "grads = torch.stack(grads)\n",
        "print(grads)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LLITapZgPME",
        "outputId": "9fa9e1da-d29e-4f13-a6cc-19546bfc5594"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 4.,  6.],\n",
            "        [12., 27.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Jacobian Derivatives\n",
        "\n",
        "For a function:\n",
        "\n",
        "$$\n",
        "\\mathbf{y} = f(\\mathbf{x}), \\quad \\mathbf{x} \\in \\mathbb{R}^n, \\mathbf{y} \\in \\mathbb{R}^m\n",
        "$$\n",
        "\n",
        "the **Jacobian** is the matrix of all partial derivatives:\n",
        "\n",
        "$$\n",
        "J = \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} = \\begin{bmatrix} \\frac{\\partial y_1}{\\partial x_1} & \\dots & \\frac{\\partial y_1}{\\partial x_n} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial y_m}{\\partial x_1} & \\dots & \\frac{\\partial y_m}{\\partial x_n} \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "-   **Rows** = output dimensions\n",
        "    \n",
        "-   **Columns** = input dimensions\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "sYLNsbRIRFxt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Simple example in PyTorch**\n",
        "\n",
        "Let’s take:\n",
        "\n",
        "$$\\mathbf{x} = \\left[\\right. x_{0} , x_{1} \\left]\\right. , \\mathbf{y} = \\left[\\right. x_{0}^{2} , x_{1}^{3} \\left]\\right.$$"
      ],
      "metadata": {
        "id": "trL69jfYVVb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "y = torch.stack([x[0]**2, x[1]**3])  # y = [4, 27]"
      ],
      "metadata": {
        "id": "SYqsddoeVN1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jacobian = []\n",
        "\n",
        "for i in range(y.shape[0]):\n",
        "    x.grad = None              # reset gradient\n",
        "    y[i].backward(retain_graph=True)\n",
        "    jacobian.append(x.grad.clone())\n",
        "\n",
        "jacobian = torch.stack(jacobian)\n",
        "print(jacobian)\n",
        "## same as we did above, but here a touch of loss functions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZahW0d6Vv_l",
        "outputId": "a34229f1-f9e7-48df-dfcf-1e6b6c7ad313"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 4.,  0.],\n",
            "        [ 0., 27.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd.functional import jacobian\n",
        "\n",
        "def f(x):\n",
        "    return torch.stack([x[0]**2, x[1]**3])\n",
        "\n",
        "J = jacobian(f, x)\n",
        "print(J)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4T4wvBeSV_b2",
        "outputId": "f0bafe64-6a86-4aed-895b-12e68144a4d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 4.,  0.],\n",
            "        [ 0., 27.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose your network outputs a vector $\\mathbf{y}_{\\text{pred}} = \\left[\\right. y_{1} , y_{2} , \\ldots , y_{n} \\left]\\right.$ and your targets are $\\mathbf{t} = \\left[\\right. t_{1} , t_{2} , \\ldots , t_{n} \\left]\\right.$.\n",
        "\n",
        "You define **per-output independent losses**:\n",
        "\n",
        "$$L_{i} = \\frac{1}{2} \\left(\\right. y_{i} - t_{i} \\left.\\right)^{2} , i = 1 , \\ldots , n$$\n",
        "\n",
        "- Each $L_{i}$ **depends only on $y_{i}$**, not on any other $y_{j}$ where $j \\neq i$."
      ],
      "metadata": {
        "id": "M_BUrv7ZTPcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The Jacobian of the vector of losses $\\mathbf{L} = [L_1, L_2, \\dots, L_n]$ w\\.r.t predictions $\\mathbf{y}_\\text{pred}$ is:\n",
        "\n",
        "$$\n",
        "J = \\frac{\\partial \\mathbf{L}}{\\partial \\mathbf{y}_\\text{pred}} =\n",
        "\\begin{bmatrix}\n",
        "\\frac{\\partial L_1}{\\partial y_1} & \\frac{\\partial L_1}{\\partial y_2} & \\dots & \\frac{\\partial L_1}{\\partial y_n} \\\\\n",
        "\\frac{\\partial L_2}{\\partial y_1} & \\frac{\\partial L_2}{\\partial y_2} & \\dots & \\frac{\\partial L_2}{\\partial y_n} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "\\frac{\\partial L_n}{\\partial y_1} & \\frac{\\partial L_n}{\\partial y_2} & \\dots & \\frac{\\partial L_n}{\\partial y_n} \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "Since each $L_i$ depends **only** on $y_i$:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L_i}{\\partial y_j} =\n",
        "\\begin{cases}\n",
        "y_i - t_i & \\text{if } i=j \\\\\n",
        "0 & \\text{if } i \\neq j\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "So all **off-diagonal entries are zero**, only the diagonal entries are non-zero.\n",
        "\n",
        "-----\n",
        "\n",
        "\n",
        "\n",
        "$$\n",
        "\\mathbf{y}_\\text{pred} = [y_1, y_2], \\quad \\mathbf{t} = [t_1, t_2]\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbf{L} = \\begin{bmatrix} L_1 \\\\ L_2 \\end{bmatrix} =\n",
        "\\begin{bmatrix} \\frac{1}{2} (y_1-t_1)^2 \\\\ \\frac{1}{2} (y_2-t_2)^2 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Jacobian:\n",
        "\n",
        "$$\n",
        "J = \\begin{bmatrix} \\frac{\\partial L_1}{\\partial y_1} & \\frac{\\partial L_1}{\\partial y_2} \\\\[2mm] \\frac{\\partial L_2}{\\partial y_1} & \\frac{\\partial L_2}{\\partial y_2} \\end{bmatrix} =\n",
        "\\begin{bmatrix} y_1 - t_1 & 0 \\\\ 0 & y_2 - t_2 \\end{bmatrix}\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "uJd16lzZhjNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "y = torch.stack([x[0]**2, x[1]**3])"
      ],
      "metadata": {
        "id": "PGaH-XnYlmm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How does python handle these jacobians?\n",
        "\n",
        "Suppose you have:\n",
        "\n",
        "$$\n",
        "\\mathbf{y} = f(\\mathbf{x}) \\in \\mathbb{R}^m, \\quad \\mathbf{x} \\in \\mathbb{R}^n\n",
        "$$\n",
        "\n",
        "* Full Jacobian: $J = \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}$ → shape `[m, n]`\n",
        "* Now, instead of computing the full Jacobian, we often compute:\n",
        "\n",
        "$$\n",
        "v^T J\n",
        "$$\n",
        "\n",
        "* Where $v \\in \\mathbb{R}^m$ is some **vector of weights**.\n",
        "\n",
        "**This is called the “vector-Jacobian product”**.\n",
        "\n",
        "---\n",
        "\n",
        "#  Simple example\n",
        "\n",
        "$$\n",
        "\\mathbf{x} = [x_0, x_1], \\quad \\mathbf{y} = [x_0^2, x_1^3]\n",
        "$$\n",
        "\n",
        "* Jacobian:\n",
        "\n",
        "$$\n",
        "J = \\begin{bmatrix} 2x_0 & 0 \\\\ 0 & 3x_1^2 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "* Pick `grad_output = [1.0, 1.0]` (this is $v$)\n",
        "\n",
        "Then **vector-Jacobian product**:\n",
        "\n",
        "$$\n",
        "v^T J = [1, 1] \\cdot\n",
        "\\begin{bmatrix} 2x_0 & 0 \\\\ 0 & 3x_1^2 \\end{bmatrix}\n",
        "= [2x_0, 3x_1^2]\n",
        "$$\n",
        "\n",
        "* Exactly what PyTorch stores in `x.grad`:\n",
        "\n",
        "---\n",
        "\n",
        "#  Why PyTorch uses this?\n",
        "\n",
        "* **Efficiency:** For large networks, storing the full Jacobian (size `m x n`) is huge.\n",
        "* **Backpropagation:** You usually have a **scalar loss L**. Scalar is equivalent to VJP with `v=1` for that output.\n",
        "* **General case:** When output is a vector, `.backward(grad_output=v)` computes **$$v^T J$$** directly without building the full Jacobian.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "e7B3tsYSkEEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward(torch.tensor([1.0, 1.0]))  ## this that v\n",
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZQ5H0RZQYsY",
        "outputId": "f008dd17-5dcc-46e3-a1aa-0fcffeba1d01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 4., 27.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  What is `nn.Module`?\n",
        "\n",
        "\n",
        "* Every **model** (big network) and **layer** (small building block) is a subclass of `nn.Module`.\n",
        "* It:\n",
        "\n",
        "  * Stores **parameters** (weights, biases).\n",
        "  * Defines a **forward pass**.\n",
        "  * Keeps track of whether you’re in **train** or **eval** mode.\n",
        "  * Lets you save/load with `.state_dict()`.\n"
      ],
      "metadata": {
        "id": "vzDGOR6vdknN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer = torch.nn.Linear(3, 2)  ## 3 input neurons and 2 output neurons"
      ],
      "metadata": {
        "id": "bPuJJQxAl0du"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(layer.parameters())  ## 6 weights, and 2 biases one for each layer, initalized random\n",
        "## requires_grad automatically true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nw2w_Hnkea6_",
        "outputId": "273593e9-b15b-46b9-b2ae-c8e2a29455ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[-0.3280,  0.2300,  0.2580],\n",
              "         [ 0.4881, -0.5599, -0.5202]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([ 0.1712, -0.4174], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# What is `forward` in PyTorch?\n",
        "\n",
        "* Every model you write in PyTorch is a **class** that inherits from `nn.Module`.\n",
        "* That class needs to say **“given an input tensor, how do I compute the output?”**\n",
        "* That’s what the `forward()` method does.\n",
        "\n",
        "In simple words:\n",
        "`forward()` = the recipe for how your network processes data.\n",
        "\n"
      ],
      "metadata": {
        "id": "o2yoCIctfuCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(4, 3)   # layer 1\n",
        "        self.fc2 = nn.Linear(3, 1)   # layer 2\n",
        "\n",
        "    def forward(self, x):            # <- define forward pass\n",
        "        x = self.fc1(x)              # linear transform\n",
        "        x = self.fc2(x)              # second linear\n",
        "        return x"
      ],
      "metadata": {
        "id": "wVTEAo54eeZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleNet()"
      ],
      "metadata": {
        "id": "NJURf4YrgPnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(model.parameters())\n",
        "## 12 weights for layer 1, and 3 biases\n",
        "## 3 weights for layer 2, and 1 bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_nYp5lcgSYh",
        "outputId": "fa070769-ce37-40ce-f7d7-b54cdb72c2e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[ 0.0808,  0.1611, -0.2435,  0.1381],\n",
              "         [-0.1649,  0.3937, -0.1798,  0.0312],\n",
              "         [-0.0459,  0.1884, -0.3284,  0.2689]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([0.1168, 0.2430, 0.1955], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([[ 0.3127,  0.0279, -0.0885]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([-0.5709], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inp = torch.randn(2, 4)   # batch of 2 samples, each with 4 features\n",
        "inp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aq_vTsIgT3J",
        "outputId": "a70a5ed4-fdcf-4efb-eaf6-73d3716d8b7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.6275,  0.3229, -0.8756, -0.2920],\n",
              "        [-0.3592, -0.0144, -0.3645,  0.6321]])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Integration with Autograd**\n",
        "\n",
        "   * When you write operations in `forward`, PyTorch builds a **computation graph** under the hood.\n",
        "   * This graph is later used by `.backward()` to compute gradients automatically.\n",
        "\n",
        "- **Consistency**\n",
        "\n",
        "   * When you do `output = model(input)`, PyTorch actually runs:\n",
        "\n",
        "     ```python\n",
        "     model.__call__(input)   # does some bookkeeping\n",
        "     -> model.forward(input) # executes your defined logic\n",
        "     ```\n",
        "\n"
      ],
      "metadata": {
        "id": "IqD2F91dhtWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model(inp) ## this calls forward\n",
        "## output of first two samples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qu5PcXIYhJnh",
        "outputId": "1ec66f82-265a-465d-bafa-f0a0c2f948d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.4753],\n",
              "        [-0.5226]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We should define all our trainable tensors inside the model, rather than in forward()"
      ],
      "metadata": {
        "id": "2ivVPeJDhshg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BadNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        fc1 = nn.Linear(10, 20)   # new layer created EACH call\n",
        "        fc2 = nn.Linear(20, 5)\n",
        "        x = torch.relu(fc1(x))\n",
        "        return fc2(x)\n"
      ],
      "metadata": {
        "id": "zOKV1fpBhSFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(BadNet().parameters()) ## no trainable tensors so empty"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcc7wD1mia65",
        "outputId": "e20562c9-7e0f-45f8-9469-b7b0e99b7fa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Activation Functions in Code\n",
        "\n",
        "## Option A: Use functional API (`torch.nn.functional`)\n",
        "\n"
      ],
      "metadata": {
        "id": "VGIceZdpjnPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(10, 20)   # 10 → 20\n",
        "        self.fc2 = nn.Linear(20, 2)    # 20 → 2 (output classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))        # ReLU activation\n",
        "        x = torch.sigmoid(x)           # Sigmoid activation\n",
        "        return x"
      ],
      "metadata": {
        "id": "LGQpv1ulie2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Option B: Define activation modules in `__init__`\n",
        "\n",
        "Both options are valid.\n",
        "\n",
        "* `F.relu` → direct function call.\n",
        "* `nn.ReLU()` → creates a module (useful in `nn.Sequential`)."
      ],
      "metadata": {
        "id": "wndinBsAj6jd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(10, 20)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(20, 2)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))     # call module like a function\n",
        "        return self.softmax(self.fc2(x))"
      ],
      "metadata": {
        "id": "NVNblnV-j1Zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Net1())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0y-Dm44Ikec1",
        "outputId": "1b0b5edb-e691-467b-d15c-afb3f3821f05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net1(\n",
            "  (fc1): Linear(in_features=10, out_features=20, bias=True)\n",
            "  (fc2): Linear(in_features=20, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Net2())  ## displays the activation ftns too"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9wOFdaXkkqA",
        "outputId": "64c09179-2064-4913-a237-2515ca684410"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net2(\n",
            "  (fc1): Linear(in_features=10, out_features=20, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (fc2): Linear(in_features=20, out_features=2, bias=True)\n",
            "  (softmax): Softmax(dim=1)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is  `nn.Sequential`?\n",
        "\n",
        "\n",
        "PyTorch has a special container class `nn.Sequential`, which lets you build a neural network by **just stacking layers one after another** — without writing your own `forward()` function.\n",
        "\n"
      ],
      "metadata": {
        "id": "fiPYvIRokx9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(10, 20),\n",
        "    nn.ReLU(),           # <- must be an nn.Module\n",
        "    nn.Linear(20, 5)\n",
        ")\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3gBx4KVkmb_",
        "outputId": "e704bf2a-a021-4e45-9e62-e169450bf7d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=10, out_features=20, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=20, out_features=5, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Here:\n",
        "\n",
        "* Each layer is numbered automatically (`0`, `1`, `2`).\n",
        "* The `ReLU` is inside the model just like any other layer.\n",
        "* You don’t need to write a `forward()` — PyTorch just applies each layer in order.\n",
        "\n",
        "That’s why for `nn.Sequential`, you **must use `nn.ReLU()` (a module)** instead of `F.relu` (a plain function), because `Sequential` only accepts modules.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "b1D2RFfFlXKD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Loss Functions in Code\n",
        "\n",
        "Loss functions are **objects** in `nn` you instantiate and call like functions:\n"
      ],
      "metadata": {
        "id": "6hW9n0SwmDgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "# Regression example\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Classification example\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "7xBL_ujfntxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### **(a) Mean Squared Error (MSE)**\n",
        "\n",
        "$$\n",
        "\\text{MSE} = \\frac{1}{N} \\sum_i (y_i - \\hat{y}_i)^2\n",
        "$$\n",
        "\n",
        "* Used in regression.\n",
        "* Sensitive to outliers.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#### **(b) Cross-Entropy Loss**\n",
        "\n",
        "For multi-class classification:\n",
        "\n",
        "$$\n",
        "\\text{CE} = -\\sum_i y_i \\log(\\text{softmax}(\\hat{y}_i))\n",
        "$$\n",
        "\n",
        "* Targets are class indices (not one-hot).\n",
        "* Combines **LogSoftmax + NLLLoss**.\n"
      ],
      "metadata": {
        "id": "zOtzar6KmgSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(4, 3)   # layer 1\n",
        "        self.fc2 = nn.Linear(3, 1)   # layer 2\n",
        "\n",
        "    def forward(self, x):            # <- define forward pass\n",
        "        x = F.relu(self.fc1(x))              # linear transform\n",
        "        x = self.fc2(x)              # second linear\n",
        "        return x"
      ],
      "metadata": {
        "id": "kkmiwi9ZnbAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "x = torch.randn(5, 4)    # batch of 5, input size 10\n",
        "y = torch.randint(0, 2, (5,1))  # target labels (0 or 1)\n",
        "\n",
        "model = SimpleNet()\n",
        "outputs = model(x)\n",
        "\n",
        "print(f\"Outputs: {outputs}\")\n",
        "print(f\"Targets: {y}\")\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "loss = criterion(outputs, y.float())   # compute loss\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bE2XDHq6mO2c",
        "outputId": "692a014b-a60d-4aaa-b29b-0c7344769d77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outputs: tensor([[0.0991],\n",
            "        [0.3812],\n",
            "        [0.1265],\n",
            "        [0.0756],\n",
            "        [0.1997]], grad_fn=<AddmmBackward0>)\n",
            "Targets: tensor([[0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0],\n",
            "        [0]])\n",
            "0.043347615748643875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "So in code:\n",
        "\n",
        "* Activations → either `F.relu(x)` or `self.relu(x)`\n",
        "* Loss functions → create with `criterion = nn.LossName()` and call with `(outputs, targets)`\n"
      ],
      "metadata": {
        "id": "MIRPGKZqn0Ax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Can we see computation graph of loss?\n",
        "\n",
        "* **Blue nodes**: trainable parameters (`fc1.weight`, `fc1.bias`, `fc2.weight`, `fc2.bias`)\n",
        "\n",
        "* **Gray nodes**: operations/backward functions\n",
        "\n",
        "  * `AddmmBackward0` → linear layer computation ($x.W^T + b$)\n",
        "  * `ReluBackward0` → ReLU non-linearity\n",
        "  * `MseLossBackward0` → final loss\n",
        "\n",
        "* **Green node**: output/loss scalar\n",
        "\n",
        "* Arrows point **backward** from loss to parameters — shows **gradient flow**.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "GdC0ZHiSsefN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchviz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsKT56dA8fi9",
        "outputId": "51449c34-f898-4f2b-8aa5-c44a9c473acb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.12/dist-packages (0.0.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from torchviz) (2.8.0+cu126)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from torchviz) (0.21)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->torchviz) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->torchviz) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchviz import make_dot\n",
        "from IPython.display import Image\n",
        "\n",
        "dot = make_dot(loss, params=dict(model.named_parameters()))\n",
        "dot.render(\"computation_graph\", format=\"png\")\n",
        "Image(\"computation_graph.png\")"
      ],
      "metadata": {
        "id": "nVV4c5iEnjXE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "outputId": "12274806-6816-424b-88b7-59beb03fd1f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**The computation graph for autograd** shows **all operations and how gradients are computed**, including:\n",
        "\n",
        "1. **Multiple inputs to the same operation**\n",
        "\n",
        "   * For example, `AddmmBackward0` receives both the previous layer’s output and the layer’s bias → two arrows pointing to it.\n",
        "\n",
        "2. **Parallel gradient paths**\n",
        "\n",
        "   * Each parameter has its own `AccumulateGrad` node where its gradient will be stored.\n",
        "   * Gradients flow back **simultaneously** through different branches.\n",
        "\n",
        "3. **Non-linearities create branching**\n",
        "\n",
        "   * ReLU doesn’t just pass forward — it also creates a `ReluBackward0` node, which is part of the gradient chain.\n",
        "\n",
        "4. **Loss connects to multiple parameters**\n",
        "\n",
        "   * The same loss depends on `fc1` and `fc2` parameters → the graph shows **branching backward paths**.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xT3R3ukwq42Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Before backward(), params : {list(model.parameters())}\")\n",
        "loss.backward()\n",
        "print(f\"\\nAfter backward(), params : {list(model.parameters())}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7IKbHy3roCs",
        "outputId": "ce176042-ccc0-4738-8fe1-43ff41251abc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before backward(), params : [Parameter containing:\n",
            "tensor([[ 0.0717, -0.3069,  0.3673, -0.1639],\n",
            "        [-0.3930, -0.2620, -0.4400,  0.2956],\n",
            "        [ 0.1346, -0.0923,  0.3324, -0.2831]], requires_grad=True), Parameter containing:\n",
            "tensor([ 0.4736, -0.4135, -0.3320], requires_grad=True), Parameter containing:\n",
            "tensor([[ 0.3155, -0.2440, -0.3907]], requires_grad=True), Parameter containing:\n",
            "tensor([0.0756], requires_grad=True)]\n",
            "\n",
            "After backward(), params : [Parameter containing:\n",
            "tensor([[ 0.0717, -0.3069,  0.3673, -0.1639],\n",
            "        [-0.3930, -0.2620, -0.4400,  0.2956],\n",
            "        [ 0.1346, -0.0923,  0.3324, -0.2831]], requires_grad=True), Parameter containing:\n",
            "tensor([ 0.4736, -0.4135, -0.3320], requires_grad=True), Parameter containing:\n",
            "tensor([[ 0.3155, -0.2440, -0.3907]], requires_grad=True), Parameter containing:\n",
            "tensor([0.0756], requires_grad=True)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# No update in weights?"
      ],
      "metadata": {
        "id": "PQjlvw-stLKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After you call `loss.backward()` in PyTorch, you’ve computed the gradients of your loss with respect to all the parameters that have `requires_grad=True`. The next steps usually involve **updating the model parameters** using these gradients and then optionally **clearing them** before the next iteration. Here's a clear step-by-step outline:\n",
        "\n",
        "---\n",
        "\n",
        "###  **Update parameters**\n",
        "\n",
        "You usually do this with an optimizer, e.g., `torch.optim.SGD` or `Adam`.\n"
      ],
      "metadata": {
        "id": "IZDdFzTNvE4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "print(f\"Before optimizer, params : {list(model.parameters())}\")\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "optimizer.step()  ## this updates weights\n",
        "print(f\"\\nAfter backward(), params : {list(model.parameters())}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Co1rF5qirteu",
        "outputId": "f25380c8-209b-4a1c-f6b2-be158e1c6e2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before optimizer, params : [Parameter containing:\n",
            "tensor([[ 0.0717, -0.3069,  0.3673, -0.1639],\n",
            "        [-0.3930, -0.2620, -0.4400,  0.2956],\n",
            "        [ 0.1346, -0.0923,  0.3324, -0.2831]], requires_grad=True), Parameter containing:\n",
            "tensor([ 0.4736, -0.4135, -0.3320], requires_grad=True), Parameter containing:\n",
            "tensor([[ 0.3155, -0.2440, -0.3907]], requires_grad=True), Parameter containing:\n",
            "tensor([0.0756], requires_grad=True)]\n",
            "\n",
            "After backward(), params : [Parameter containing:\n",
            "tensor([[ 0.0715, -0.3071,  0.3668, -0.1640],\n",
            "        [-0.3930, -0.2620, -0.4400,  0.2956],\n",
            "        [ 0.1344, -0.0922,  0.3337, -0.2824]], requires_grad=True), Parameter containing:\n",
            "tensor([ 0.4726, -0.4135, -0.3314], requires_grad=True), Parameter containing:\n",
            "tensor([[ 0.3135, -0.2440, -0.3907]], requires_grad=True), Parameter containing:\n",
            "tensor([0.0720], requires_grad=True)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **Basic idea of optimizers**\n",
        "\n",
        "After computing gradients:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\text{loss}}{\\partial \\theta_i} \\quad \\text{for each parameter } \\theta_i\n",
        "$$\n",
        "\n",
        "The optimizer updates the parameters. For example, in **stochastic gradient descent (SGD)**:\n",
        "\n",
        "$$\n",
        "\\theta_i \\gets \\theta_i - \\eta \\frac{\\partial \\text{loss}}{\\partial \\theta_i}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $\\eta$ = learning rate\n",
        "* $\\theta_i$ = model parameter\n",
        "* $\\frac{\\partial \\text{loss}}{\\partial \\theta_i}$ = gradient\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Common optimizers**\n",
        "\n",
        "* **SGD** (Stochastic Gradient Descent) – simplest, can use momentum.\n",
        "* **Adam** – adapts learning rates per parameter, popular for deep learning.\n",
        "* **RMSprop** – good for recurrent networks, adjusts learning rates based on recent gradients.\n",
        "* **Adagrad** – adapts learning rates, favors infrequent features.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Why we use optimizers**\n",
        "\n",
        "* Gradients tell **direction**, but optimizers decide **how much and how exactly to move**.\n",
        "* Some optimizers speed up convergence or stabilize training.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "Zkb2YTR3vRAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "vQdWQS1mvRwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " If you don’t call `optimizer.zero_grad()` (or `model.zero_grad()`), the next iteration’s gradients will be added to the previous ones instead of replacing them. This is usually **not what you want** during standard training.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EArCTuocwMWa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets & Dataloaders"
      ],
      "metadata": {
        "id": "kE5KjSVh-h6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In deep learning, we rarely work with **one big tensor of all data** — that would be inefficient and often too large for memory.\n",
        "\n",
        "Instead, training happens in **mini-batches** (chunks of data).\n",
        "So we need a mechanism to:\n",
        "\n",
        "* Split data into batches\n",
        "* Shuffle it for randomness\n",
        "* Load it efficiently (even from disk)\n",
        "\n",
        "That’s exactly what **Dataset** and **DataLoader** handle.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZhQ0dvVl-m3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "NPoSrHOZwIxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  `Dataset` — defines how to access your data\n",
        "\n",
        "A **Dataset** is like a wrapper around your actual data. It tells PyTorch:\n",
        "\n",
        "* **How many samples** you have (`__len__`)\n",
        "* **How to get one sample** (`__getitem__`)\n",
        "\n",
        "You can use **built-in** datasets (like MNIST, CIFAR10), or make your **own custom one**.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "WKSWDosW-6ny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = datasets.MNIST(\n",
        "    root='./data', train=True,      ## give the train data not test, make False to get test data\n",
        "    transform=transforms.ToTensor(),\n",
        "    download=True\n",
        ")\n",
        "\n",
        "# Here, train_ds knows how to give you one (image, label) pair"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTa4XDeH-0hk",
        "outputId": "33743f27-8f96-4ec6-b13d-bf016a487ac1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 34.3MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.06MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 9.66MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.76MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for image, target in train_ds:\n",
        "    print(f\"Image shape: {image.shape}, label: {target}\")\n",
        "    break\n",
        "\n",
        "## MNIST has each image of 28*28 and is grayscale so only 1 channel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRxZDEEc_Ttc",
        "outputId": "55a7f088-945c-410f-84cc-6cb5a4444e3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape: torch.Size([1, 28, 28]), label: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "###  Custom dataset\n",
        "\n",
        "You can create your own:\n",
        "\n"
      ],
      "metadata": {
        "id": "rIrq-dP2_-K_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "CHVPrtgD_acV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Now:\n"
      ],
      "metadata": {
        "id": "2J9zsX0IANW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.randn(100, 10)\n",
        "y = torch.randint(0, 2, (100,))\n",
        "\n",
        "torch.randn()\n",
        "\n",
        "data = MyDataset(X, y)\n",
        "print(len(data))       # total samples\n",
        "print(data[0])         # first sample\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m65H2EZFAJMr",
        "outputId": "40f44dcf-3a30-4279-d5e8-1b42098b2217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "(tensor([-1.2597,  2.5302, -1.6251,  1.9022,  0.1483, -0.7836, -0.7778,  1.1459,\n",
            "         0.3462,  0.2734]), tensor(1))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `DataLoader` — handles batching and shuffling\n",
        "\n",
        "`DataLoader` wraps a Dataset and automatically:\n",
        "\n",
        "* Splits it into **mini-batches**\n",
        "* **Shuffles** data every epoch (if you want)\n",
        "* Can **load in parallel** using multiple workers (faster I/O)\n",
        "\n",
        "Example:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Be_FxfKXA24U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "loader = DataLoader(data, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "id": "HoWFc9g_AZyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Then in training:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F_psV1OCBDBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, target in loader:\n",
        "    print(inputs.shape, target.shape)\n",
        "    break\n",
        "\n",
        "## this is how you get data in a loop, with batching/shuffling"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-Faa0hEBCva",
        "outputId": "b0096e5f-0748-4fac-d408-d290c576bb82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 10]) torch.Size([16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Head over to next notebook for further expansion"
      ],
      "metadata": {
        "id": "lpD-Ue6KB-t2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jVX9EB_-6t_V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}